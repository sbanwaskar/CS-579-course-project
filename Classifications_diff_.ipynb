{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifications-diff-.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbanwaskar/CS-579-course-project/blob/main/Classifications_diff_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozm8M8Tr__Y2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer \n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB7dRpddAC8j"
      },
      "source": [
        "# #data import\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "!wget https://www.dropbox.com/s/xkozscpcryu871h/labels_1540_4classes_icmla_21.pck\n",
        "!wget https://www.dropbox.com/s/ovt8g99wjcz1mvr/mvts_1540_icmla_21.pck\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRLzevlnAEvH"
      },
      "source": [
        "#reading pickle files\n",
        "def load(file_name):\n",
        "    with open(file_name, 'rb') as fp:\n",
        "        obj = pickle.load(fp)\n",
        "    return obj\n",
        "\n",
        "\n",
        "Sampled_inputs=load(\"mvts_1540_icmla_21.pck\")\n",
        "\n",
        "\n",
        "Sampled_labels=load(\"labels_1540_4classes_icmla_21.pck\")    \n",
        "\n",
        "temp=Sampled_inputs[0]\n",
        "print(temp)\n",
        "df = pd.DataFrame(temp)\n",
        "trainData=Sampled_inputs\n",
        "trainLabel=Sampled_labels\n",
        "print(\"trainData.shape: \",trainData.shape)\n",
        "print(\"trainLebel.shape: \",trainLabel.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #standardization/z normalization of the univaraite time series\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc = StandardScaler()\n",
        "# npArrays=[]\n",
        "# for l in range(0, len(trainData)):\n",
        "#   trainData_std = sc.fit_transform(trainData[l])\n",
        "#   #trainData_std = trainData_std.astype(np.float64)\n",
        "#   #print(type(trainData_std[0][0]))\n",
        "#   npArrays.append(trainData_std)\n",
        "\n",
        "# print(type(npArrays))\n",
        "# arr = np.asarray(npArrays)\n",
        "# print(type(arr))\n",
        "# trainData=arr\n",
        "# print(\"trainData.shape: \",trainData.shape)\n",
        "# print(type(trainData))\n",
        "# print(\"trainLebel.shape: \",trainLabel.shape)\n",
        "# print(type(trainLabel))"
      ],
      "metadata": {
        "id": "SyPv8JBTUVRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P12kQOtsCBzA"
      },
      "source": [
        "Saving all data to csv (different files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzz4x5g9Ads3"
      },
      "source": [
        "def time_series(trainData):\n",
        "\n",
        "  rowList = []\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range(0, len(trainData)):\n",
        "\n",
        "    for j in range(0, len(trainData[i])):\n",
        "      \n",
        "      rowList.append(trainData[i][j])\n",
        "  # print(len(rowList))\n",
        "  return rowList\n",
        "\n",
        "def time_series1(rowList):\n",
        "  rows, cols = (len(rowList), len(rowList[0])-1)\n",
        "\n",
        "# here we declare a row list to which we append the values for all the 8 parameters.\n",
        "  rowList1 = [[0 for i in range(cols)] for j in range(rows)]\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range (0, len(rowList)):\n",
        "\n",
        "    for j in range(0,len(rowList[i])-1):\n",
        "\n",
        "      rowList1[i][j] = rowList[i][j+1] - rowList[i][j]\n",
        "  return rowList1\n",
        "\n",
        "def get_mean_std(rowList):\n",
        "  Mean = np.array(rowList).mean(1)\n",
        "  means = np.reshape(Mean, (1540, 33))\n",
        "  standardDeviation = np.array(rowList1).std(1)\n",
        "  stdDev = np.reshape(standardDeviation, (1540, 33))\n",
        "  return means,stdDev\n",
        "\n",
        "def get_skew_and_kurtosis(rowList):\n",
        "  df = pd.DataFrame(rowList)\n",
        "  skew_df = df.skew(1)\n",
        "  kurtosis_df = df.kurtosis(1)\n",
        "  skew_rs = np.reshape(skew_df.values,(1540, 33))\n",
        "  kurtosis_rs = np.reshape(kurtosis_df.values,(1540, 33))\n",
        "  return skew_rs , kurtosis_rs\n",
        "\n",
        "def get_all_params(rowList):\n",
        "  mean , std_dev =get_mean_std(rowList)\n",
        "  skew , kurtosis = get_skew_and_kurtosis(rowList)\n",
        "  return mean,std_dev, skew,kurtosis\n",
        "\n",
        "rowList = time_series(trainData)\n",
        "rowList1 = time_series1(rowList)\n",
        "\n",
        "mean,std_dev,skew , kurtosis = get_all_params(rowList)\n",
        "mean1,std_dev1,skew1 , kurtosis1 = get_all_params(rowList1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtprlAu_AfnM"
      },
      "source": [
        "all_params = [mean, std_dev,skew , kurtosis,mean1,std_dev1,skew1 , kurtosis1]\n",
        "all_param_names = ['mean','std_dev','skew','kurtosis','mean1','std_dev1','skew1','kurtosis1']\n",
        "\n",
        "\n",
        "## Method 1 : All Data in one csv please follow all_param_names to check the sequencing of data\n",
        "# write\n",
        "all_arrays_list = [a for a in all_params]\n",
        "all_arrays_np = np.array(all_arrays_list).reshape(1540*8 , 33)\n",
        "np.savetxt(\"all_params_data.csv\", all_arrays_np, delimiter=\",\")\n",
        "# read\n",
        "data_read_back = np.loadtxt('all_params_data.csv',delimiter=',')\n",
        "all_param_names = ['mean','std_dev','skew','kurtosis','mean1','std_dev1','skew1','kurtosis1']\n",
        "all_param_data = data_read_back.reshape(8, 1540 , 33)\n",
        "\n",
        "\n",
        "## Method 2 : All in one excel with different sheets (more organized, better when checking for issues in data by reading the excel manually)\n",
        "# write-only\n",
        "df_list = [pd.DataFrame(val) for val in all_params] \n",
        "filename = 'all_params_excel'\n",
        "writer = pd.ExcelWriter(filename+'.xlsx')\n",
        "for i, df in enumerate(df_list):\n",
        "    df.to_excel(writer,sheet_name=all_param_names[i])\n",
        "writer.save() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkDIMViIAhnW"
      },
      "source": [
        "# All in one compressed pickle\n",
        "\n",
        "final_data = {'mean':mean,\n",
        "              'std_dev':std_dev,\n",
        "              'skew':skew,\n",
        "              'kurtosis':kurtosis,\n",
        "              'mean1':mean1,\n",
        "              'std_dev1':std_dev1,\n",
        "              'skew1':skew1,\n",
        "              'kurtosis1':kurtosis1}\n",
        "\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "\n",
        "filename = 'all_data_in_one'\n",
        "\n",
        "def decompress_pickle(file):\n",
        "  data = bz2.BZ2File(file, 'rb')\n",
        "  data = cPickle.load(data)\n",
        "  return data\n",
        "\n",
        "def compressed_pickle(title, data):\n",
        "  with bz2.BZ2File(title + '.pbz2', 'w') as f: \n",
        "   cPickle.dump(data, f)\n",
        "\n",
        "compressed_pickle(filename, final_data) \n",
        "data = decompress_pickle(filename+'.pbz2') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TooKlLsTAls2"
      },
      "source": [
        "!zip -r all_data_in_one.zip all_*\n",
        "from google.colab import files\n",
        "files.download('all_data_in_one.zip') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujjUEBjsAqzw"
      },
      "source": [
        "def time_series():\n",
        "\n",
        "  rowList = []\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range(0, len(trainData)):\n",
        "\n",
        "    for j in range(0, len(trainData[i])):\n",
        "      \n",
        "      rowList.append(trainData[i][j])\n",
        "  # print(len(rowList))\n",
        "  return rowList\n",
        "\n",
        "def time_series1(rowList):\n",
        "  rows, cols = (len(rowList), len(rowList[0])-1)\n",
        "\n",
        "# here we declare a row list to which we append the values for all the 8 parameters.\n",
        "  rowList1 = [[0 for i in range(cols)] for j in range(rows)]\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range (0, len(rowList)):\n",
        "\n",
        "    for j in range(0,len(rowList[i])-1):\n",
        "\n",
        "      rowList1[i][j] = rowList[i][j+1] - rowList[i][j]\n",
        "  return rowList1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv4NftybAsXt"
      },
      "source": [
        "def mean():\n",
        "  Mean = np.array(rowList).mean(1)\n",
        "  means = np.reshape(Mean, (1540, 33))\n",
        "  df = pd.DataFrame(means)\n",
        "  print(means)\n",
        "  df.to_csv(\"mean.csv\")\n",
        "def deviation():\n",
        "  standardDeviation = np.array(rowList).std(1)\n",
        "  stdDev = np.reshape(standardDeviation, (1540, 33))\n",
        "  DF = pd.DataFrame(stdDev)\n",
        "  print(\"info for deviation\")\n",
        "  print(DF.info)\n",
        "  DF.to_csv(\"standardDeviation.csv\")\n",
        "def  skew():\n",
        "  df = pd.DataFrame(rowList)\n",
        "  skewValue = np.array(df.skew(1))\n",
        "  skews = np.reshape(skewValue, (1540, 33))\n",
        "  df = pd.DataFrame(skews)\n",
        "  print(\"info for skew\")\n",
        "  print(df.info)\n",
        "  df.to_csv(\"skew.csv\")\n",
        "def kurt():\n",
        "  df = pd.DataFrame(rowList)\n",
        "  kurtosisValue = np.array(df.kurt(1))\n",
        "  kurts = np.reshape(kurtosisValue, (1540, 33))\n",
        "  df = pd.DataFrame(kurts)\n",
        "  print(\"info for kurtsosis\")\n",
        "  print(df.info)\n",
        "  df.to_csv(\"kurt.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JG1Y3YDAuC9"
      },
      "source": [
        "def mean():\n",
        "  Mean = np.array(rowList).mean(1)\n",
        "  means = np.reshape(Mean, (1540, 33))\n",
        "  df = pd.DataFrame(means)\n",
        "  print(means)\n",
        "  df.to_csv(\"mean.csv\")\n",
        "def deviation():\n",
        "  standardDeviation = np.array(rowList).std(1)\n",
        "  stdDev = np.reshape(standardDeviation, (1540, 33))\n",
        "  DF = pd.DataFrame(stdDev)\n",
        "  print(\"info for deviation\")\n",
        "  print(DF.info)\n",
        "  DF.to_csv(\"standardDeviation.csv\")\n",
        "def  skew():\n",
        "  df = pd.DataFrame(rowList)\n",
        "  skewValue = np.array(df.skew(1))\n",
        "  skews = np.reshape(skewValue, (1540, 33))\n",
        "  df = pd.DataFrame(skews)\n",
        "  print(\"info for skew\")\n",
        "  print(df.info)\n",
        "  df.to_csv(\"skew.csv\")\n",
        "def kurt():\n",
        "  df = pd.DataFrame(rowList)\n",
        "  kurtosisValue = np.array(df.kurt(1))\n",
        "  kurts = np.reshape(kurtosisValue, (1540, 33))\n",
        "  df = pd.DataFrame(kurts)\n",
        "  print(\"info for kurtsosis\")\n",
        "  print(df.info)\n",
        "  df.to_csv(\"kurt.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpUSnbHUAvkB"
      },
      "source": [
        "def time_series1(rowList):\n",
        "  rows, cols = (len(rowList), len(rowList[0])-1)\n",
        "\n",
        "# here we declare a row list to which we append the values for all the 8 parameters.\n",
        "  rowList1 = [[0 for i in range(cols)] for j in range(rows)]\n",
        "#Here we parse through the entire trainData matrix and append eveything to the row list.\n",
        "  for i in range (0, len(rowList)):\n",
        "\n",
        "    for j in range(0,len(rowList[i])-1):\n",
        "\n",
        "      rowList1[i][j] = rowList[i][j+1] - rowList[i][j]\n",
        "\n",
        "  return rowList1\n",
        "    #print(len(rowList))\n",
        "# the size of the row list is 1540*33 = 50820.\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyDg9imaAy2q"
      },
      "source": [
        "Mean = np.array(rowList1).mean(1)\n",
        "means = np.reshape(Mean, (1540, 33))\n",
        "df = pd.DataFrame(means)\n",
        "df.to_csv(\"mean.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2b8TeZWA0eP"
      },
      "source": [
        "def mean1():\n",
        "  Mean = np.array(rowList1).mean(1)\n",
        "  means = np.reshape(Mean, (1540, 33))\n",
        "  df = pd.DataFrame(means)\n",
        "  df.to_csv(\"mean1.csv\")\n",
        "def deviation1():\n",
        "  standardDeviation = np.array(rowList1).std(1)\n",
        "  stdDev = np.reshape(standardDeviation, (1540, 33))\n",
        "  DF = pd.DataFrame(stdDev)\n",
        "  DF.to_csv(\"standardDeviation1.csv\")\n",
        "def skew1():\n",
        "  df = pd.DataFrame(rowList1)\n",
        "  skewValue = np.array(df.skew(1))\n",
        "\n",
        "#print(len(skewValue))\n",
        "  skews = np.reshape(skewValue, (1540, 33))\n",
        "  df = pd.DataFrame(skews)\n",
        "  df.to_csv(\"skew1.csv\")\n",
        "def kurt1():\n",
        "  df = pd.DataFrame(rowList1)\n",
        "  kurtosisValue = np.array(df.kurt(1))\n",
        "#print(len(skewVale))\n",
        "  kurts = np.reshape(kurtosisValue, (1540, 33))\n",
        "  df = pd.DataFrame(kurts)\n",
        "  df.to_csv(\"kurt1.csv\")\n",
        "#size of kurtosis is 1540*33\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZz3g5mJO8q"
      },
      "source": [
        "def main_mean():\n",
        " df = pd.read_csv(\"all_params_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYFmd5agK4wn"
      },
      "source": [
        "main_mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import loguniform\n",
        "from pandas import read_csv\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "GE1ond-SW7xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJK6IgaQ_7u1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c937e85-af84-40ac-f6c7-956e5fc3f2e3"
      },
      "source": [
        "#Here change the name of the file in [''] to check accuracy of required file\n",
        "\t# ['mean'] ['std_dev'] ['skew'] ['kurtosis'] ['mean1'] ['std_dev1'] ['skew1'] ['kurtosis']\n",
        "X = decompress_pickle('all_data_in_one.pbz2')['kurtosis1']\n",
        "Y = trainLabel\n",
        "X.shape,Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1540, 33), (1540,))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bh2GtYFIda0"
      },
      "source": [
        "**KNN Classification**\n",
        "**/SVM**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rurXBRGkEuOB"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2A0sJfrP7eo"
      },
      "source": [
        " from sklearn.model_selection import train_test_split\n",
        " X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30)\n",
        " from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CF7nNixFJMd"
      },
      "source": [
        "#using polynomial kernel\n",
        "from sklearn.svm import SVC\n",
        "svclassifier_polynomail = SVC(kernel='poly', degree=8)\n",
        "svclassifier_polynomail.fit(X_train, Y_train)\n",
        "#Y_pred = svclassifier.predict(X_test)\n",
        "Y_pred = svclassifier_polynomail.predict(X_test)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph6vBkkx80d5"
      },
      "source": [
        " #using gaussian rbf kernel\n",
        "svclassifier_gauss = SVC(kernel='rbf',random_state=0)\n",
        "svclassifier_gauss.fit(X_train, Y_train)\n",
        "Y_pred = svclassifier_gauss.predict(X_test)\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sruq5KT9UVh"
      },
      "source": [
        "#using sigmoid kernel\n",
        "\n",
        "svclassifier_sigmoid = SVC(kernel='sigmoid',random_state=0)\n",
        "svclassifier_sigmoid.fit(X_train, Y_train)\n",
        "Y_pred = svclassifier_sigmoid.predict(X_test)\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukj-PqPB9kTe"
      },
      "source": [
        "#using linear kernel\n",
        "svclassifier_linear = SVC(kernel='linear')\n",
        "svclassifier_linear.fit(X_train, Y_train)\n",
        "Y_pred = svclassifier_linear.predict(X_test)\n",
        "print(confusion_matrix(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function importing Dataset\n",
        "\n",
        "# Function to split the dataset\n",
        "def splitdataset(balance_data):\n",
        "\n",
        "\t# Separating the target variable\n",
        "\t#Here change the name of the file in [''] to check accuracy of required file\n",
        "\t# ['mean'] ['std_dev'] ['skew'] ['kurtosis'] ['mean1'] ['std_dev1'] ['skew1'] ['kurtosis']\n",
        "\tX = decompress_pickle('all_data_in_one.pbz2')['std_dev1']\n",
        "\tY = trainLabel\n",
        "\n",
        "\t# Splitting the dataset into train and test\n",
        "\tX_train, X_test, y_train, y_test = train_test_split(\n",
        "\tX, Y, test_size = 0.3, random_state = 100)\n",
        "\t\n",
        "\treturn X, Y, X_train, X_test, y_train, y_test\n",
        "\t\n",
        "# Function to perform training with giniIndex.\n",
        "def train_using_gini(X_train, X_test, y_train):\n",
        "\n",
        "\t# Creating the classifier object\n",
        "\tclf_gini = DecisionTreeClassifier(criterion = \"gini\",\n",
        "\t\t\trandom_state = 100,max_depth=3, min_samples_leaf=5)\n",
        "\n",
        "\t# Performing training\n",
        "\tclf_gini.fit(X_train, y_train)\n",
        "\treturn clf_gini\n",
        "\t\n",
        "# Function to perform training with entropy.\n",
        "def tarin_using_entropy(X_train, X_test, y_train):\n",
        "\n",
        "\t# Decision tree with entropy\n",
        "\tclf_entropy = DecisionTreeClassifier(\n",
        "\t\t\tcriterion = \"entropy\", random_state = 100,\n",
        "\t\t\tmax_depth = 3, min_samples_leaf = 5)\n",
        "\n",
        "\t# Performing training\n",
        "\tclf_entropy.fit(X_train, y_train)\n",
        "\treturn clf_entropy\n",
        "\n",
        "\n",
        "# Function to make predictions\n",
        "def prediction(X_test, clf_object):\n",
        "\n",
        "\t# Predicton on test with giniIndex\n",
        "\ty_pred = clf_object.predict(X_test)\n",
        "\tprint(\"Predicted values:\")\n",
        "\tprint(y_pred)\n",
        "\treturn y_pred\n",
        "\t\n",
        "# Function to calculate accuracy\n",
        "def cal_accuracy(y_test, y_pred):\n",
        "\t\n",
        "\tprint(\"Confusion Matrix: \",\n",
        "\t\tconfusion_matrix(y_test, y_pred))\n",
        "\t\n",
        "\tprint (\"Accuracy : \",\n",
        "\taccuracy_score(y_test,y_pred)*100)\n",
        "\t\n",
        "\tprint(\"Report : \",\n",
        "\tclassification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "def main():\n",
        "\t\n",
        "  #Here change the name of the file in [''] to check accuracy of required file\n",
        "\t# ['mean'] ['std_dev'] ['skew'] ['kurtosis'] ['mean1'] ['std_dev1'] ['skew1'] ['kurtosis']\n",
        "\tdata = decompress_pickle('all_data_in_one.pbz2')['std_dev1']\n",
        "\tX, Y, X_train, X_test, y_train, y_test = splitdataset(data)\n",
        "\tclf_gini = train_using_gini(X_train, X_test, y_train)\n",
        "\tclf_entropy = tarin_using_entropy(X_train, X_test, y_train)\n",
        "\t\n",
        "\t# Operational Phase\n",
        "\tprint(\"Results Using Gini Index:\")\n",
        "\t\n",
        "\t# Prediction using gini\n",
        "\ty_pred_gini = prediction(X_test, clf_gini)\n",
        "\tcal_accuracy(y_test, y_pred_gini)\n",
        "\t\n",
        "\tprint(\"Results Using Entropy:\")\n",
        "\t# Prediction using entropy\n",
        "\ty_pred_entropy = prediction(X_test, clf_entropy)\n",
        "\tcal_accuracy(y_test, y_pred_entropy)\n",
        "\t\n",
        "\t\n",
        "# Calling main function\n",
        "if __name__==\"__main__\":\n",
        "\tmain()\n"
      ],
      "metadata": {
        "id": "P7BniHI4qNf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KNN Classification\n"
      ],
      "metadata": {
        "id": "mYSnHf3dhTbg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dgSrzaIOEsP"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y, test_size = 0.30,random_state = 17)\n",
        "#X_train, Y_test = train_test_split(df, test_size = 0.20, random_state= 17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.fit_transform(X_test)\n",
        "# X_train =  scaler.transform(X_train)\n",
        "# X_test  =  scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "W4QmXDbmhq2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "\n",
        "lab_enc = preprocessing.LabelEncoder()\n",
        "encoded = lab_enc.fit_transform(Y_train)\n",
        "\n",
        "\n",
        "print(utils.multiclass.type_of_target(Y_train))\n",
        "\n",
        "print(utils.multiclass.type_of_target(Y_train.astype('int')))\n",
        "\n",
        "\n",
        "print(utils.multiclass.type_of_target(encoded))\n"
      ],
      "metadata": {
        "id": "o1Ev1xE4htXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "import seaborn as sns\n",
        "def get_knn(X_train,Y_train,X_test,Y_test,n_neighbors):\n",
        "  neigh = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "  neigh.fit(X_train, Y_train)\n",
        "  accuracy = accuracy_score(neigh.predict(X_test),Y_test)\n",
        "  print('Accuracy:',accuracy,'\\n\\n')\n",
        "  print(classification_report(neigh.predict(X_test),Y_test))\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "nrmL5czlhwTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list = []\n",
        "n_neighbor_list = []\n",
        "accuracy_std_list = []\n",
        "for i in range(1,11):\n",
        "  print(\"KNN for n= \",i,\" after flattening - (1540 * 8 x 33) :\\n\")\n",
        "  accuracy = get_knn(X_train,Y_train,X_test,Y_test,n_neighbors=i)\n",
        "  print(\"KNN after standardization  for n= \",i,\":\\n\")\n",
        "  accuracy_std = get_knn(X_train_std,Y_train,X_test_std,Y_test,n_neighbors=i)\n",
        "  accuracy_list.append(accuracy)\n",
        "  n_neighbor_list.append(i)\n",
        "  accuracy_std_list.append(accuracy_std)"
      ],
      "metadata": {
        "id": "laerZ6zPh0kg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}